# Transformer æ¶æ„è¯¾ç¨‹

> é€šè¿‡äº¤äº’å¼å¯è§†åŒ–,æ·±å…¥ç†è§£ Transformer æ¶æ„åŸç†

## ğŸ“š è¯¾ç¨‹ç®€ä»‹

æœ¬è¯¾ç¨‹é€šè¿‡ **13 ä¸ªæ¨¡å—**çš„äº¤äº’å¼å¯è§†åŒ–,å¸®åŠ©ä½ æ·±å…¥ç†è§£ Transformer æ¶æ„çš„æ ¸å¿ƒåŸç†ã€‚

### æ ¸å¿ƒç‰¹ç‚¹

- âœ… **äº¤äº’å¼å¯è§†åŒ–** - åŠ¨ç”»ã€å›¾è¡¨ã€3Dæ¨¡å‹,å¤šç»´åº¦å±•ç¤º
- âœ… **æ— éœ€ç¼–ç¨‹åŸºç¡€** - é€šè¿‡å¯è§†åŒ–ç†è§£,ä¸ä¾èµ–ä»£ç å®ç°
- âœ… **è‡ªè¿›åº¦å­¦ä¹ ** - å¯åå¤æŸ¥çœ‹,è°ƒæ•´å‚æ•°è§‚å¯Ÿæ•ˆæœ
- âœ… **å®Œå…¨å…è´¹** - æ‰€æœ‰å†…å®¹å¼€æº,éšæ—¶å­¦ä¹ 

## ğŸ¯ å­¦ä¹ è·¯å¾„

### ç¬¬ä¸€é˜¶æ®µ:ç†è®ºåŸºç¡€ (34-36)

| æ¨¡å— | ä¸»é¢˜ | éš¾åº¦ | æ—¶é•¿ |
|------|------|------|------|
| [34](34-transformer-background.html) | Transformer è¯ç”ŸèƒŒæ™¯ | å…¥é—¨ | ~50åˆ†é’Ÿ |
| [35](35-self-attention.html) | Self-Attention è¯¦è§£ | å…¥é—¨ | ~60åˆ†é’Ÿ |
| [36](36-multi-head-attention.html) | Multi-Head Attention | è¿›é˜¶ | ~60åˆ†é’Ÿ |

**å­¦ä¹ ç›®æ ‡:**
- ç†è§£ä¸ºä»€ä¹ˆéœ€è¦ Transformer
- æŒæ¡ Self-Attention çš„è®¡ç®—è¿‡ç¨‹
- ç†è§£ Multi-Head å’Œä½ç½®ç¼–ç çš„ä½œç”¨

### ç¬¬äºŒé˜¶æ®µ:æ¶æ„ç»„ä»¶ (37-40)

| æ¨¡å— | ä¸»é¢˜ | éš¾åº¦ | æ—¶é•¿ |
|------|------|------|------|
| [37](37-feedforward-addnorm.html) | Feed-Forward & Add&Norm | è¿›é˜¶ | ~55åˆ†é’Ÿ |
| [38](38-encoder-architecture.html) | Encoder æ¶æ„æ·±åº¦è§£æ | è¿›é˜¶ | ~65åˆ†é’Ÿ |
| [39](39-decoder-architecture.html) | Decoder æ¶æ„æ·±åº¦è§£æ | è¿›é˜¶ | ~65åˆ†é’Ÿ |
| [40](40-complete-transformer.html) | å®Œæ•´ Transformer æ¨¡å‹ | è¿›é˜¶ | ~70åˆ†é’Ÿ |

**å­¦ä¹ ç›®æ ‡:**
- ç†è§£ Encoder å’Œ Decoder çš„å†…éƒ¨ç»“æ„
- æŒæ¡æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–çš„ä½œç”¨
- ç†è§£å®Œæ•´çš„ Transformer æ¶æ„

### ç¬¬ä¸‰é˜¶æ®µ:æ¨¡å‹å˜ä½“ (41-43)

| æ¨¡å— | ä¸»é¢˜ | éš¾åº¦ | æ—¶é•¿ |
|------|------|------|------|
| [41](41-bert-series.html) | BERT ç³»åˆ—æ¨¡å‹ | è¿›é˜¶ | ~65åˆ†é’Ÿ |
| [42](42-gpt-series.html) | GPT ç³»åˆ—æ¨¡å‹ | è¿›é˜¶ | ~65åˆ†é’Ÿ |
| [43](43-other-variants.html) | å…¶ä»–é‡è¦å˜ä½“ | è¿›é˜¶ | ~60åˆ†é’Ÿ |

**å­¦ä¹ ç›®æ ‡:**
- ç†è§£ BERT å’Œ GPT çš„æ¶æ„å·®å¼‚
- æŒæ¡é¢„è®­ç»ƒå’Œå¾®è°ƒçš„æ¦‚å¿µ
- äº†è§£ T5ã€BARTã€ViT ç­‰å˜ä½“

### ç¬¬å››é˜¶æ®µ:é«˜çº§ä¸»é¢˜ (44-46)

| æ¨¡å— | ä¸»é¢˜ | éš¾åº¦ | æ—¶é•¿ |
|------|------|------|------|
| [44](44-training-optimization.html) | è®­ç»ƒæŠ€å·§ä¸ä¼˜åŒ– | é«˜çº§ | ~70åˆ†é’Ÿ |
| [45](45-efficient-transformers.html) | é«˜æ•ˆ Transformer å˜ä½“ | é«˜çº§ | ~65åˆ†é’Ÿ |
| [46](46-applications.html) | Transformer åº”ç”¨å®æˆ˜ | é«˜çº§ | ~75åˆ†é’Ÿ |

**å­¦ä¹ ç›®æ ‡:**
- äº†è§£è®­ç»ƒä¼˜åŒ–æŠ€å·§
- ç†è§£é«˜æ•ˆ Transformer å˜ä½“
- æŒæ¡å®é™…åº”ç”¨åœºæ™¯

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ä¸€:åœ¨çº¿å­¦ä¹ 

ç›´æ¥åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ HTML æ–‡ä»¶å³å¯å¼€å§‹å­¦ä¹ :

```bash
# è¿›å…¥è¯¾ç¨‹ç›®å½•
cd transformer

# æ‰“å¼€æ€»è§ˆé¡µ
start index.html  # Windows
open index.html   # macOS
xdg-open index.html  # Linux
```

### æ–¹å¼äºŒ:æœ¬åœ°æœåŠ¡å™¨

æ¨èä½¿ç”¨æœ¬åœ°æœåŠ¡å™¨ä»¥è·å¾—æœ€ä½³ä½“éªŒ:

```bash
# ä½¿ç”¨ Python 3
cd transformer
python -m http.server 8000

# ç„¶ååœ¨æµè§ˆå™¨ä¸­è®¿é—®
# http://localhost:8000
```

## ğŸ“– å­¦ä¹ å»ºè®®

### å¾ªåºæ¸è¿›
- æŒ‰ç…§æ¨¡å—é¡ºåºå­¦ä¹ ,æ¯ä¸ªæ¨¡å—å»ºç«‹åœ¨å‰é¢çš„çŸ¥è¯†åŸºç¡€ä¸Š
- å…ˆæŒæ¡ç†è®ºåŸºç¡€(34-36),å†å­¦ä¹ æ¶æ„ç»„ä»¶(37-40)

### äº¤äº’å¼å­¦ä¹ 
- ä¸è¦åªçœ‹,è¦åŠ¨æ‰‹æ“ä½œå¯è§†åŒ–ç»„ä»¶
- è°ƒæ•´å‚æ•°,è§‚å¯Ÿç»“æœå˜åŒ–
- å°è¯•ä¸åŒçš„è¾“å…¥,ç†è§£æ¨¡å‹è¡Œä¸º

### ç†è§£æ ¸å¿ƒæ¦‚å¿µ
- **Self-Attention**: ç†è§£ Q/K/V çš„å«ä¹‰å’Œè®¡ç®—è¿‡ç¨‹
- **Multi-Head**: ç†è§£ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´æ³¨æ„åŠ›
- **Position Encoding**: ç†è§£ä½ç½®ä¿¡æ¯çš„ç¼–ç æ–¹å¼
- **Encoder vs Decoder**: ç†è§£ä¸¤ç§æ¶æ„çš„å·®å¼‚
- **BERT vs GPT**: ç†è§£ä¸¤ç§é¢„è®­ç»ƒèŒƒå¼çš„åŒºåˆ«

## ğŸ¨ å¯è§†åŒ–ç±»å‹

æœ¬è¯¾ç¨‹åŒ…å«å¤šç§äº¤äº’å¼å¯è§†åŒ–:

### 1. æ¶æ„å›¾
- å¯ç‚¹å‡»çš„ç»„ä»¶å±•ç¤ºè¯¦ç»†ä¿¡æ¯
- æ•°æ®æµåŠ¨çš„åŠ¨ç”»æ¼”ç¤º
- ç¼©æ”¾ã€æ—‹è½¬æŸ¥çœ‹ç»†èŠ‚

### 2. çŸ©é˜µè¿ç®—åŠ¨ç”»
- é€æ­¥å±•ç¤ºè®¡ç®—è¿‡ç¨‹
- é«˜äº®æ˜¾ç¤ºä¸­é—´ç»“æœ
- äº¤äº’å¼è°ƒæ•´ç»´åº¦

### 3. Attention Map çƒ­åŠ›å›¾
- å®æ—¶æ˜¾ç¤ºæ³¨æ„åŠ›æƒé‡
- å¯è¾“å…¥æ–‡æœ¬æŸ¥çœ‹ attention
- Hover æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

### 4. å¯¹æ¯”æ¼”ç¤º
- Before/After å¯¹æ¯”
- å‚æ•°è°ƒæ•´çš„å®æ—¶åé¦ˆ
- ä¸åŒæ¨¡å‹çš„æ¶æ„å¯¹æ¯”

## ğŸ› ï¸ æŠ€æœ¯æ ˆ

- **æ ·å¼**: Tailwind CSS
- **å¯è§†åŒ–**: D3.js / ECharts
- **åŠ¨ç”»**: CSS3 Transitions / JavaScript
- **3Då¯è§†åŒ–**: Three.js (å¯é€‰)

## ğŸ“ ç›®å½•ç»“æ„

```
transformer/
â”œâ”€â”€ index.html                          # è¯¾ç¨‹æ€»è§ˆé¡µ
â”œâ”€â”€ 34-transformer-background.html      # æ¨¡å— HTML é¡µé¢
â”œâ”€â”€ 35-self-attention.html
â”œâ”€â”€ ...
â”œâ”€â”€ 46-applications.html
â”œâ”€â”€ 34-transformer-background.py        # Python æ¼”ç¤ºæ–‡ä»¶(å¯é€‰)
â”œâ”€â”€ ...
â”œâ”€â”€ assets/                             # å¯è§†åŒ–èµ„æº
â”‚   â”œâ”€â”€ js/                             # JavaScript åº“
â”‚   â”œâ”€â”€ css/                            # è‡ªå®šä¹‰æ ·å¼
â”‚   â””â”€â”€ data/                           # é¢„è®¡ç®—çš„æ•°æ®
â””â”€â”€ README.md                           # æœ¬æ–‡ä»¶
```

## ğŸ’¡ å¸¸è§é—®é¢˜

### Q: éœ€è¦ä»€ä¹ˆåŸºç¡€?
A: æ— éœ€ç¼–ç¨‹åŸºç¡€,ä½†å¯¹ä»¥ä¸‹æ¦‚å¿µæœ‰å¸®åŠ©:
- åŸºæœ¬çš„æ•°å­¦æ¦‚å¿µ(çŸ©é˜µä¹˜æ³•ã€å‘é‡)
- æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µ(è®­ç»ƒã€æ¨ç†ã€æ¢¯åº¦ä¸‹é™)
- æ·±åº¦å­¦ä¹ åŸºç¡€æ¦‚å¿µ(ç¥ç»ç½‘ç»œã€å±‚ã€æ¿€æ´»å‡½æ•°)

### Q: ä¸ºä»€ä¹ˆé€‰æ‹©äº¤äº’å¼å¯è§†åŒ–?
A: Transformer æ¶æ„å¤æ‚,çº¯æ–‡å­—æˆ–ä»£ç éš¾ä»¥ç†è§£ã€‚å¯è§†åŒ–èƒ½:
- ç›´è§‚å±•ç¤ºæ•°æ®æµåŠ¨
- åŠ¨æ€æ¼”ç¤ºè®¡ç®—è¿‡ç¨‹
- äº¤äº’å¼æ¢ç´¢ä¸åŒå‚æ•°çš„æ•ˆæœ
- é™ä½å­¦ä¹ é—¨æ§›

### Q: éœ€è¦å®‰è£…ä¾èµ–å—?
A: ä¸éœ€è¦!æ‰€æœ‰å†…å®¹éƒ½æ˜¯çº¯ HTML/CSS/JavaScript,ç›´æ¥åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€å³å¯ã€‚

### Q: å¯ä»¥è·³è¿‡æŸäº›æ¨¡å—å—?
A: å¯ä»¥,ä½†ä¸å»ºè®®ã€‚æ¨¡å—ä¹‹é—´æœ‰ä¾èµ–å…³ç³»,æŒ‰é¡ºåºå­¦ä¹ æ•ˆæœæœ€å¥½ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æå‡ºå»ºè®®å’Œæ”¹è¿›!

### å¦‚ä½•è´¡çŒ®?
1. Fork æœ¬é¡¹ç›®
2. åˆ›å»ºä½ çš„ç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤ä½ çš„ä¿®æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## ğŸ“š å‚è€ƒèµ„æº

### è®ºæ–‡
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer åŸå§‹è®ºæ–‡
- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

### æ•™ç¨‹
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [The Illustrated BERT](http://jalammar.github.io/illustrated-bert/)
- [Visualizing A Machine Learning Model](https://distill.pub/2016/foreword-for-ml/)

### å·¥å…·
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- [Tensor2Tensor](https://github.com/tensorflow/tensor2tensor)

## ğŸ“„ è®¸å¯

MIT License

---

**ç¥ä½ å­¦ä¹ æ„‰å¿«!å¦‚æœ‰é—®é¢˜,éšæ—¶è¯¢é—®ã€‚** ğŸ‰
